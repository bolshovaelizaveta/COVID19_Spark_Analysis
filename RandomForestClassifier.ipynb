{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd7d583-4fcb-406e-b987-8feb0e9ff869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Заполнены пропуски в leukocyte_count средним 5.02\n",
      "Заполнены пропуски в neutrophil_count средним 5.31\n",
      "Заполнены пропуски в lymphocyte_count средним 4.64\n",
      "Начинаем разделение данных на обучающую и тестовую выборки...\n",
      "Обучающая выборка: 797 строк\n",
      "Тестовая выборка: 153 строк\n",
      "Начинаем обучение модели Random Forest...\n",
      "Обучение модели Random Forest завершено.\n",
      "Выполняем предсказания на тестовой выборке...\n",
      "+---------+--------+----------+-----------------------------------------+\n",
      "|patientid|is_covid|prediction|probability                              |\n",
      "+---------+--------+----------+-----------------------------------------+\n",
      "|112      |1       |0.0       |[0.6978567191645906,0.3021432808354095]  |\n",
      "|114      |1       |0.0       |[0.6099254138726917,0.3900745861273082]  |\n",
      "|115      |1       |0.0       |[0.6099254138726917,0.3900745861273082]  |\n",
      "|117      |1       |0.0       |[0.6248392613673083,0.3751607386326916]  |\n",
      "|12       |1       |1.0       |[0.1102325694937897,0.8897674305062102]  |\n",
      "|123      |1       |1.0       |[0.01882704707754584,0.9811729529224542] |\n",
      "|129      |1       |1.0       |[0.01158898969242963,0.9884110103075703] |\n",
      "|13       |1       |1.0       |[0.04336321248891425,0.9566367875110858] |\n",
      "|134      |1       |1.0       |[0.016820204196537257,0.9831797958034627]|\n",
      "|135      |1       |1.0       |[0.018642287027649144,0.9813577129723509]|\n",
      "+---------+--------+----------+-----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Оцениваем производительность модели Random Forest...\n",
      "Area Under ROC (AUC) на тестовой выборке (Random Forest): 0.9054\n",
      "\n",
      "Важность признаков (Random Forest):\n",
      "  temperature: 0.0403\n",
      "  lymphocyte_count: 0.0386\n",
      "  view_encoded: 0.0177\n",
      "  pO2_saturation: 0.0175\n",
      "  survival_encoded: 0.0149\n",
      "  age_numeric: 0.0110\n",
      "  sex_encoded: 0.0106\n",
      "  RT_PCR_positive_encoded: 0.0100\n",
      "  age_group_encoded: 0.0088\n",
      "  modality_encoded: 0.0067\n",
      "  leukocyte_count: 0.0019\n",
      "  neutrophil_count: 0.0001\n",
      "SparkSession остановлена. Обучение и оценка ML-модели Random Forest завершены.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce, lit, mean as _mean \n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Инициализация SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"COVID19_ML_Model_RandomForest\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "optimized_parquet_path = \"hdfs:///covid_dataset/metadata_optimized/\"\n",
    "\n",
    "# Путь в HDFS, где хранятся очищенные и оптимизированные метаданные в формате Parquet\n",
    "df_ml = spark.read.parquet(optimized_parquet_path)\n",
    "\n",
    "# Целевая переменная: is_covid (0 или 1)\n",
    "# Признаки: sex, age_group, view, modality, RT_PCR_positive, survival, temperature, pO2_saturation, leukocyte_count, neutrophil_count, lymphocyte_count\n",
    "\n",
    "# Список категориальных признаков, которые нужно индексировать и One-Hot-кодировать\n",
    "categorical_features = [\n",
    "    \"sex\",\n",
    "    \"age_group\",\n",
    "    \"view\",\n",
    "    \"modality\",\n",
    "    \"RT_PCR_positive\",\n",
    "    \"survival\"\n",
    "]\n",
    "\n",
    "# Список числовых признаков\n",
    "numeric_features = [\n",
    "    \"age_numeric\",\n",
    "    \"temperature\",\n",
    "    \"pO2_saturation\",\n",
    "    \"leukocyte_count\",\n",
    "    \"neutrophil_count\",\n",
    "    \"lymphocyte_count\"\n",
    "]\n",
    "\n",
    "# Валидация и заполнение пропусков в числовых признаках\n",
    "for nf in numeric_features:\n",
    "    if nf in df_ml.columns:\n",
    "        if df_ml.filter(col(nf).isNull()).count() > 0:\n",
    "            avg_val = df_ml.select(_mean(col(nf))).collect()[0][0]\n",
    "            if avg_val is not None:\n",
    "                df_ml = df_ml.withColumn(nf, col(nf).cast(\"double\"))\n",
    "                df_ml = df_ml.na.fill(avg_val, subset=[nf])\n",
    "                print(f\"Заполнены пропуски в {nf} средним {avg_val:.2f}\")\n",
    "            else:\n",
    "                print(f\"Предупреждение: Колонка {nf} содержит только NULL значения, невозможно заполнить средним. Заполняем 0.0.\")\n",
    "                df_ml = df_ml.withColumn(nf, lit(0.0).cast(\"double\"))\n",
    "    else:\n",
    "        print(f\"Предупреждение: Числовая колонка '{nf}' не найдена в DataFrame. Удаляем из списка.\")\n",
    "        numeric_features.remove(nf) # Удаляем несуществующие колонки из списка\n",
    "\n",
    "# Создаем стадии Pipeline для StringIndexer и OneHotEncoderEstimator\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=feature, outputCol=feature + \"_indexed\", handleInvalid=\"keep\")\n",
    "    for feature in categorical_features if feature in df_ml.columns\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=feature + \"_indexed\", outputCol=feature + \"_encoded\")\n",
    "    for feature in categorical_features if feature in df_ml.columns\n",
    "]\n",
    "\n",
    "assembler_inputs = [f.getOutputCol() for f in encoders] + numeric_features\n",
    "\n",
    "if not assembler_inputs:\n",
    "    raise ValueError(\"Не найдено действительных признаков для VectorAssembler после всех преобразований. Проверьте списки признаков и схему DataFrame.\")\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "# RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"is_covid\", numTrees=100, maxDepth=10, seed=42) \n",
    "\n",
    "# Создание Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [vector_assembler, rf]) \n",
    "\n",
    "print(\"Начинаем разделение данных на обучающую и тестовую выборки...\")\n",
    "# Разделение данных на обучающую 80% и тестовую 20% \n",
    "(training_data, test_data) = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Обучающая выборка: {training_data.count()} строк\")\n",
    "print(f\"Тестовая выборка: {test_data.count()} строк\")\n",
    "\n",
    "print(\"Начинаем обучение модели Random Forest...\")\n",
    "# Обучение модели\n",
    "model = pipeline.fit(training_data)\n",
    "print(\"Обучение модели Random Forest завершено.\")\n",
    "\n",
    "# Предсказания на тестовой выборке\n",
    "print(\"Выполняем предсказания на тестовой выборке...\")\n",
    "predictions = model.transform(test_data)\n",
    "predictions.select(\"patientid\", \"is_covid\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# Оценка модели\n",
    "print(\"Оцениваем производительность модели Random Forest...\")\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"is_covid\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Area Under ROC (AUC) на тестовой выборке (Random Forest): {auc:.4f}\")\n",
    "\n",
    "# Важность признаков для Random Forest\n",
    "try:\n",
    "    rf_model = model.stages[-1] \n",
    "    if hasattr(rf_model, 'featureImportances'):\n",
    "        print(\"\\nВажность признаков (Random Forest):\")\n",
    "        importances = rf_model.featureImportances.toArray()\n",
    "        # Сопоставим важность с именами признаков\n",
    "        feature_names = vector_assembler.getInputCols()\n",
    "        # Создаем список пар (признак, важность) и сортируем по важности\n",
    "        feature_importance_pairs = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
    "        for feature, importance in feature_importance_pairs:\n",
    "            print(f\"  {feature}: {importance:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nВажность признаков недоступна для этой модели Random Forest.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nНе удалось получить важность признаков Random Forest: {e}\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"SparkSession остановлена. Обучение и оценка ML-модели Random Forest завершены.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
